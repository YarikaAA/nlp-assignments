{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Arina Yartseva BS21-AAI-01\n",
        "\n",
        "a.yartseva@innopolis.university "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvig’s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import re\n",
        "from collections import Counter\n",
        "from functools import cache\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import difflib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "random.seed(21)\n",
        "np.random.seed(21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "words_lambda = lambda text: re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words_lambda(open('big.txt').read()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NGram:\n",
        "    def __init__(self, filename: str):\n",
        "        \"\"\"\n",
        "        Initialize the n-gram model\n",
        "\n",
        "        :param filename: the filename of the n-gram model\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(filename, header=None, delimiter='\\t', encoding='latin-1')\n",
        "        self.voco = set(self.df[1]).union(set(self.df[2]))\n",
        "        self.sum_all = self.df[0].sum()\n",
        "\n",
        "    @property\n",
        "    def n(self):\n",
        "        \"\"\"\n",
        "        Get the n of the n-gram\n",
        "\n",
        "        :return: the n of the n-gram\n",
        "        \"\"\"\n",
        "        # The n-gram order is determined by the number of word columns in the dataframe.\n",
        "        return len(self.df.columns) - 1\n",
        "\n",
        "    @cache\n",
        "    def get_prob(self, context: str, word: str) -> float:\n",
        "        \"\"\"\n",
        "        Get the probability of a word given a context\n",
        "\n",
        "        :param context: the context\n",
        "        :param word: the word\n",
        "        :return: the probability of the word given the context\n",
        "        \"\"\"\n",
        "        # Filter the dataframe for rows where the context matches.\n",
        "        context_df = self.df[self.df[1] == context]\n",
        "\n",
        "        # If the context is not found, return a uniform probability.\n",
        "        if not len(context_df):\n",
        "            return 1.0 / self.sum_all\n",
        "        \n",
        "        # Filter for the specific word within the matching context.\n",
        "        word_context = context_df[context_df.iloc[:, -1] == word]\n",
        "        word_context_s = 1.0\n",
        "\n",
        "        # If the word is found within the context, adjust the score based on frequency.\n",
        "        if len(word_context):\n",
        "            word_context_s += float(word_context.iloc[0, 0])\n",
        "\n",
        "        # Return the normalized probability.\n",
        "        return word_context_s / self.sum_all\n",
        "\n",
        "    def get_sentence_probability(self, sentence: list[str]) -> float:\n",
        "        \"\"\"\n",
        "        Get the probability of a sentence\n",
        "\n",
        "        :param sentence: the sentence\n",
        "        :return: the probability of the sentence\n",
        "        \"\"\"\n",
        "        result = 1.0\n",
        "\n",
        "        # Iterate through the sentence to calculate probabilities for each n-gram within it.\n",
        "        for i in range(len(sentence) - self.n + 1):\n",
        "            a, b = sentence[i: i + self.n]\n",
        "            r = self.get_prob(a, b)\n",
        "            result *= r\n",
        "            \n",
        "        # Return the cumulative probability of the sentence.\n",
        "        return result\n",
        "    \n",
        "\n",
        "n2g = NGram('w2_.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_vocab = set(WORDS.keys())\n",
        "vocob = n2g.voco.union(unique_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "E1_coef = 0.9999999\n",
        "E2_coef = 0.0000001\n",
        "Ecorr_coef = 700000.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def known(words: set[str]) -> set[str]:\n",
        "    \"\"\"\n",
        "    The subset of `words` that appear in the dictionary of WORDS.\n",
        "    \n",
        "    :param words: the words\n",
        "    :return: the subset of `words` that appear in the dictionary of WORDS\n",
        "    \"\"\"\n",
        "    return set(w for w in words if w in WORDS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_unigram_prob_in_edits(edits: set[str], coef: float) -> dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate the probability of a word given the edits\n",
        "\n",
        "    :param edits: the edits\n",
        "    :param coef: the coefficient\n",
        "    :return: the probability of a word given the edits\n",
        "    \"\"\"\n",
        "    known_words = []\n",
        "    known_probs = []\n",
        "\n",
        "    # Loop through each edit and, if it's known, calculate its probability.\n",
        "    for edit in known(edits):\n",
        "        known_words.append(edit)\n",
        "        known_probs.append(WORDS.get(edit, 0))\n",
        "    \n",
        "    # Convert the list of probabilities to a numpy array for vectorized operations.\n",
        "    known_probs = np.array(known_probs)\n",
        "    \n",
        "    # Apply Laplace smoothing (adding 1 to each count) and multiply by the given coefficient.\n",
        "    # Laplace smoothing is used to handle zero probabilities for unseen words.\n",
        "    known_probs = (known_probs + 1) * coef\n",
        "    \n",
        "    # Return a dictionary mapping each known word (edit) to its adjusted probability.\n",
        "    return {word: prob for word, prob in zip(known_words, known_probs)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def norm_values(d : dict[str, float]) -> dict[str, float]:\n",
        "    \"\"\"\n",
        "    Normalize the values of a dictionary\n",
        "\n",
        "    :param d: the dictionary\n",
        "    :return: the normalized dictionary\n",
        "    \"\"\"\n",
        "    total = sum(d.values())\n",
        "    for k in d:\n",
        "        d[k] = d[k] / total\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "@cache\n",
        "def edits1(word: str) -> set[str]:\n",
        "    \"\"\"\n",
        "    All edits that are one edit away from `word`.\n",
        "\n",
        "    :param word: the word\n",
        "    :return: all edits that are one edit away from `word`\n",
        "    \"\"\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word: str) -> set[str]:\n",
        "    \"\"\"\n",
        "    All edits that are two edits away from `word`.\n",
        "    \n",
        "    :param word: the word\n",
        "    :return: all edits that are two edits away from `word`\n",
        "    \"\"\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def candidates(word: str):\n",
        "    \"\"\"\n",
        "    Generate possible spelling corrections for word.\n",
        "\n",
        "    :param word: the word\n",
        "    :return: the possible spelling corrections for word\n",
        "    \"\"\"\n",
        "    if len(word) == 1:\n",
        "        return Counter({word: 1.0})\n",
        "\n",
        "    known_probs = {}\n",
        "\n",
        "    # Generate and update probabilities for corrections that are two edits away (for words longer than 3 characters).\n",
        "    if len(word) > 3:\n",
        "        known_probs.update(calc_unigram_prob_in_edits(edits2(word), E2_coef))\n",
        "\n",
        "    # Generate and update probabilities for corrections that are one edit away.\n",
        "    known_probs.update(calc_unigram_prob_in_edits(edits1(word), E1_coef))\n",
        "    \n",
        "    # If the original word is known, assign it a high probability.\n",
        "    if word in WORDS:\n",
        "        known_probs.update(calc_unigram_prob_in_edits({word}, Ecorr_coef))\n",
        "\n",
        "    # If no known corrections are found, return the original word as the only correction.\n",
        "    if not known_probs:\n",
        "        return Counter({word: 1.0})\n",
        "\n",
        "    # Normalize the probabilities of the corrections and return them.\n",
        "    return Counter(norm_values(known_probs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "BeamSize = 4\n",
        "\n",
        "def correct_sentence(sentence: str) -> str:\n",
        "    \"\"\"\n",
        "    Correct a sentence\n",
        "\n",
        "    :param sentence: the sentence\n",
        "    :return: the corrected sentence\n",
        "    \"\"\"\n",
        "    # Tokenize the input sentence and convert it to lowercase\n",
        "    tokenized = word_tokenize(sentence.lower())\n",
        "    \n",
        "    # Initialize candidates for the first word\n",
        "    current_candidates = candidates(tokenized[0])\n",
        "    current_candidates = current_candidates.most_common(BeamSize)\n",
        "    \n",
        "    # Iterate over the rest of the words in the sentence\n",
        "    for i in tqdm(range(1, len(tokenized)), desc=\"Correcting sentence\"):\n",
        "        # Generate candidates for the next word\n",
        "        next_candidates = candidates(tokenized[i])\n",
        "        next_candidates = next_candidates.most_common(BeamSize)\n",
        "        \n",
        "        # Prepare to collect new candidate combinations\n",
        "        new_candidates = {}\n",
        "        \n",
        "        # Combine each pair of current and next candidates\n",
        "        for first_word, first_prob in current_candidates:\n",
        "            last_word_of_first = first_word.split(' ')[-1]\n",
        "            for second_word, second_prob in next_candidates:\n",
        "                # Calculate the combined probability of the two-word sequence\n",
        "                ng_prob = n2g.get_prob(last_word_of_first, second_word)\n",
        "                combined_prob = first_prob * ng_prob * second_prob\n",
        "                new_sequence = first_word + ' ' + second_word\n",
        "                new_candidates[new_sequence] = combined_prob\n",
        "        \n",
        "        # Select the top BS candidates based on their combined probabilities\n",
        "        current_candidates = Counter(new_candidates).most_common(BeamSize)\n",
        "    \n",
        "    # Return the most probable corrected sentence\n",
        "    return current_candidates[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc.\n",
        "\n",
        "In this implementation, several decisions were made regarding the n-gram dataset, weights for edit probabilities, and beam search parameters. Here, we give the justifications of these decisions.\n",
        "\n",
        "N-gram dataset: The model used in this implementation treats words as occurring in pairs. Therefore, this dataset was chosen because it allows for a reasonable compromise between context capture and computation efficiency. Our 5-gram dataset is too small to be of use for this model.\n",
        "\n",
        "Edit probabilities weights: The respective weights given to the edit probabilities are E1_coef = 0.9999999 and E2_coef = 0.0000001. This is on the general assumption that single edit (edit1) corrections are much more probable than double-edit (edit2) ones. This is the reasonable assumption, since in most cases, spelling mistakes are more likely simple typographical errors or single-character errors. Ecorr_coef is 700000.0 in order to give a far higher probability to the known original word, should it be known. Then, it is the probable correct spelling for the original word. The coefficients were empirically chosen by the test set.\n",
        "\n",
        "Search parameters (use beam search): search procedure attempts to find the most likely sequences of corrections for the given sentence. The beam size was taken as equal to 4. The beam size was set empirically, using a test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Norvig corrector\n",
        "https://norvig.com/spell-correct.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open('big.txt').read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())):\n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word):\n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(n_candidates(word), key=P)\n",
        "\n",
        "def n_candidates(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (n_known([word]) or n_known(n_edits1(word)) or n_known(n_edits2(word)) or [word])\n",
        "\n",
        "def n_known(words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def n_edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def n_edits2(word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in n_edits1(word) for e2 in n_edits1(e1))\n",
        "\n",
        "def correct_sentence_norvig(sentence):\n",
        "    tokeinzed = word_tokenize(sentence.lower())\n",
        "    corr = [correction(e) for e in tokeinzed]\n",
        "    return corr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test set generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"Holmes had sat up upon the couch, and I saw him motion like a man who is in need of air. A maid rushed across and threw open the window. At the same instant I saw him raise his hand and at the signal I tossed my rocket into the room with a cry of Fire! The word was no sooner out of my mouth than the whole crowd of spectators, well dressed and ill--gentlemen, ostlers, and servant maids--joined in a general shriek of Fire! Thick clouds of smoke curled through the room and out at the open window. I caught a glimpse of rushing figures, and a moment later the voice of Holmes from within assuring them that it was a false alarm. Slipping through the shouting crowd I made my way to the corner of the street, and in ten minutes was rejoiced to find my friend's arm in mine, and to get away from the scene of uproar. He walked swiftly and in silence for some few minutes until we had turned down one of the quiet streets which lead towards the Edgeware Road.\"\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "sentences = [re.sub(r'[^a-zA-Z ]', '', e).lower() for e in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def deletion(L: str, R: str, c: str) -> str:\n",
        "    \"\"\"\n",
        "    Perform deletion edit: Remove the first character from R.\n",
        "    \n",
        "    :param L: the left part of the string\n",
        "    :param R: the right part of the string\n",
        "    :param c: the character\n",
        "    :return: the string after deletion\n",
        "    \"\"\"\n",
        "    return L + R[1:] if R and R[0] != ' ' else L + R\n",
        "\n",
        "def transposition(L: str, R: str, c: str) -> str:\n",
        "    \"\"\"\n",
        "    Perform transposition edit: Swap the first two characters in R.\n",
        "    \n",
        "    :param L: the left part of the string\n",
        "    :param R: the right part of the string\n",
        "    :param c: the character\n",
        "    :return the string after transposition\n",
        "    \"\"\"\n",
        "    return L + R[1] + R[0] + R[2:] if len(R) > 1 else L + R\n",
        "\n",
        "def replacement(L: str, R: str, c: str) -> str:\n",
        "    \"\"\"\n",
        "    Perform replacement edit: Replace the first character in R with c.\n",
        "    \n",
        "    :param L: the left part of the string\n",
        "    :param R: the right part of the string\n",
        "    :param c: the character to replace\n",
        "    \"\"\"\n",
        "    return L + c + R[1:] if R else L + c\n",
        "\n",
        "def insertion(L: str, R: str, c: str) -> str:\n",
        "    \"\"\"\n",
        "    Perform insertion edit: Insert c before R.\n",
        "    \n",
        "    :param L: the left part of the string\n",
        "    :param R: the right part of the string\n",
        "    :param c: the character to insert\n",
        "    \"\"\"\n",
        "    return L + c + R\n",
        "\n",
        "# List of operation functions\n",
        "ops = [deletion, transposition, replacement, insertion]\n",
        "\n",
        "letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "\n",
        "def error_maker(text, coef):\n",
        "    \"\"\"\n",
        "    Simulate typing errors in a given text based on a specified coefficient.\n",
        "\n",
        "    :param text: The original text to introduce errors into.\n",
        "    :param coef: A coefficient indicating the fraction of the text to be modified with errors.\n",
        "    :return: The modified text with simulated typing errors.\n",
        "    \"\"\"\n",
        "    num_errors = int(len(text) * coef)\n",
        "    new_text = text\n",
        "\n",
        "    for _ in range(num_errors):\n",
        "        # Choose a random position in the text, avoiding leading or trailing spaces for certain operations\n",
        "        pos = random.randrange(len(new_text))\n",
        "        \n",
        "        # Select a random operation and a random letter for replacement or insertion\n",
        "        op = random.choice(ops)\n",
        "        random_letter = random.choice(letters)\n",
        "        \n",
        "        # Split the text at the chosen position\n",
        "        L, R = new_text[:pos], new_text[pos:]\n",
        "        \n",
        "        # Apply the operation\n",
        "        new_text = op(L, R, random_letter)\n",
        "\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_similarity(text1, text2):\n",
        "    \"\"\"\n",
        "    Calculate the similarity ratio between two texts using SequenceMatcher.\n",
        "    \n",
        "    :param text1: The first text for comparison.\n",
        "    :param text2: The second text for comparison.\n",
        "    :return: A float representing the similarity ratio, where 1.0 is identical, and 0 is completely different.\n",
        "    \"\"\"\n",
        "    sequence_matcher = difflib.SequenceMatcher(a=text1, b=text2)\n",
        "    return sequence_matcher.ratio()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "erroneous_sentences = [error_maker(sentence, 0.1) for sentence in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "similarity_product_custom_correction = 1.0\n",
        "similarity_product_norvig_correction = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Correcting sentence:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Correcting sentence: 100%|██████████| 20/20 [00:05<00:00,  3.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Erroneous: olmes had sat up uopwn thez couch and i saw him motiop like a ma nhwo is icn need of air\n",
            "Corrected (Custom): holmes had sat up down the couch and i saw him motion like a a who is in need of air\n",
            "Corrected (Norvig): holmes had sat up down the couch and i saw him motion like a ma who is in need of air\n",
            "Similarity (Custom): 0.9647058823529412\n",
            "Similarity (Norvig): 0.9707602339181286\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Correcting sentence: 100%|██████████| 8/8 [00:02<00:00,  2.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Erroneous: a maid aushed across wand tbhrew open the windw\n",
            "Corrected (Custom): a maid rushed across and threw open the window\n",
            "Corrected (Norvig): a maid rushed across wand threw open the window\n",
            "Similarity (Custom): 1.0\n",
            "Similarity (Norvig): 0.989247311827957\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Correcting sentence: 100%|██████████| 23/23 [00:07<00:00,  2.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Erroneous: at the same instant m saw him raise his han dand at the signali  tossedr my rocket int he room witha  ry gof fier\n",
            "Corrected (Custom): at the same instant m saw him raise his man and at the signal tossed my rocket in the room with by of fire\n",
            "Corrected (Norvig): at the same instant m saw him raise his had and at the signal tossed my rocket in he room with by of fire\n",
            "Similarity (Custom): 0.9363636363636364\n",
            "Similarity (Norvig): 0.9406392694063926\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Correcting sentence: 100%|██████████| 27/27 [00:06<00:00,  3.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Erroneous: the wod was no sonoer out fo my mouth thn klhe whole crowd of spectators well dressed anh illgentlemen lstlers andk servanti vmidsjoinde in a general shriekrof ire\n",
            "Corrected (Custom): the word was no sooner out of my mouth the the whole crowd of spectators well dressed and illgentlemen ostlers and servants vmidsjoinde in a general shriekrof ire\n",
            "Corrected (Norvig): the god was no sooner out fo my mouth the the whole crowd of spectators well dressed and illgentlemen ostlers and servants vmidsjoinde in a general shriekrof ire\n",
            "Similarity (Custom): 0.9661538461538461\n",
            "Similarity (Norvig): 0.9506172839506173\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Correcting sentence: 100%|██████████| 13/13 [00:04<00:00,  3.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Erroneous: thick cluuds of smoke curled through he room and oubt at the open windo\n",
            "Corrected (Custom): thick clouds of smoke curled through the room and out at the open window\n",
            "Corrected (Norvig): thick clouds of smoke curled through he room and out at the open window\n",
            "Similarity (Custom): 1.0\n",
            "Similarity (Norvig): 0.993006993006993\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Correcting sentence: 100%|██████████| 23/23 [00:07<00:00,  3.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Erroneous: i vcaught a glimpse of rushing fguraesatnd a moment lazer the voice ofh olmes from iwthin assuring them thav sit was a falee alarm\n",
            "Corrected (Custom): i caught a glimpse of rushing fguraesatnd a moment later the voice of holmes from within assuring them that it was a false alarm\n",
            "Corrected (Norvig): i caught a glimpse of rushing fguraesatnd a moment later the voice of holmes from within assuring them that sit was a false alarm\n",
            "Similarity (Custom): 0.984375\n",
            "Similarity (Norvig): 0.980544747081712\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Correcting sentence: 100%|██████████| 35/35 [00:12<00:00,  2.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Erroneous: sloipping htrougyh the shotuing crowd  made ym way to the cwrner of the street mnd in ten minutes aws reojiced to find my friend sam in mine nd t oget awax from thez seene of uproar\n",
            "Corrected (Custom): slipping through the shouting crowd made my way to the corner of the street and in ten minutes as rejoiced to find my friend sam in mine and t get away from the scene of uproar\n",
            "Corrected (Norvig): slipping through the shouting crowd made my way to the corner of the street and in ten minutes was rejoiced to find my friend sam in mine nd t get away from the seen of uproar\n",
            "Similarity (Custom): 0.9803921568627451\n",
            "Similarity (Norvig): 0.9719101123595506\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Correcting sentence: 100%|██████████| 24/24 [00:09<00:00,  2.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Erroneous: he waplked sbwiftlvq and in ilence for soxe few minutes until ww had turned dsown onejfo the quiet streets whih lead towards the edgemwar eroad\n",
            "Corrected (Custom): he walked sbwiftlvq and in silence for some few minutes until we had turned down onejfo the quiet streets which lead towards the edgeware road\n",
            "Corrected (Norvig): he walked sbwiftlvq and in silence for some few minutes until we had turned down onejfo the quiet streets which lead towards the edgeware road\n",
            "Similarity (Custom): 0.9716312056737588\n",
            "Similarity (Norvig): 0.9716312056737588\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for erroneous_sentence, correct_sentence_text in zip(erroneous_sentences, sentences):\n",
        "    # Correct the sentence using the custom correction algorithm\n",
        "    corrected_custom = ''.join(correct_sentence(erroneous_sentence))\n",
        "    similarity_custom = calculate_similarity(correct_sentence_text, corrected_custom)\n",
        "    similarity_product_custom_correction *= similarity_custom\n",
        "\n",
        "    # Correct the sentence using Norvig's correction algorithm\n",
        "    corrected_norvig = ' '.join(correct_sentence_norvig(erroneous_sentence))\n",
        "    similarity_norvig = calculate_similarity(correct_sentence_text, corrected_norvig)\n",
        "    similarity_product_norvig_correction *= similarity_norvig\n",
        "\n",
        "    # Optionally, uncomment to print sentences and their similarity scores\n",
        "    print(f\"Erroneous: {erroneous_sentence}\\nCorrected (Custom): {corrected_custom}\\nCorrected (Norvig): {corrected_norvig}\")\n",
        "    print(f\"Similarity (Custom): {similarity_custom}\\nSimilarity (Norvig): {similarity_norvig}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Similarity Product (Custom Correction): 0.8183660635543504\n",
            "Final Similarity Product (Norvig Correction): 0.7895741981728733\n"
          ]
        }
      ],
      "source": [
        "print(f\"Final Similarity Product (Custom Correction): {similarity_product_custom_correction}\")\n",
        "print(f\"Final Similarity Product (Norvig Correction): {similarity_product_norvig_correction}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The final computed similarity products are as follows:\n",
        "\n",
        "Custom Correction: 0.818\n",
        "\n",
        "Norvig's Correction: 0.789\n",
        "\n",
        "This data indicates that the Custom Correction method outperforms Norvig's Correction method, achieving a higher similarity product. Specifically, the Custom Correction method achieves a similarity product of approximately 0.818, whereas Norvig's Correction method achieves about 0.789. This comparison suggests that the Custom Correction method is more effective in aligning the corrected sentences closer to the target sentences when evaluated using the similarity product metric.\n",
        "\n",
        "However, it's important to approach the comparison of these results with an understanding of the potential influences on performance. Factors such as the characteristics of the test dataset and the foundational assumptions of each algorithm could significantly impact their respective accuracies. This underscores the necessity of a nuanced evaluation when comparing the efficacy of different spelling correction implementations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### For extra checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Correcting sentence: 100%|██████████| 7/7 [00:01<00:00,  3.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: Elephants love dancing on rainbows after midnight.\n",
            "Erroneous: Elaphnts lvoe dancng on ranibows afer mdinight.\n",
            "Corrected: elephants love dancing on rainbow after midnight .\n",
            "Similarity Score: 0.96\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "original_sentence = \"Elephants love dancing on rainbows after midnight.\"\n",
        "erroneous_sentence = \"Elaphnts lvoe dancng on ranibows afer mdinight.\"\n",
        "\n",
        "corrected_sentence = ''.join(correct_sentence(erroneous_sentence))\n",
        "\n",
        "print(f\"Original: {original_sentence}\")\n",
        "print(f\"Erroneous: {erroneous_sentence}\")\n",
        "print(f\"Corrected: {corrected_sentence}\")\n",
        "\n",
        "similarity_score = calculate_similarity(original_sentence, corrected_sentence)\n",
        "print(f\"Similarity Score: {similarity_score}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
